{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "\n",
    "## 0.1 Readings\n",
    "\n",
    "A good resource for today's task (besides the assigned readings from last week) is the following tutorial: [Word2Vec Tutorial: The SkipGram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). Part 2 provides a really good overview of negative sampling!\n",
    "\n",
    "## 0.2 Task\n",
    "\n",
    "This week your tasks will be:\n",
    "1. Train a CBOW model with a real world dataset, explore how the parameters affect the model.\n",
    "2. Learn how to evaluate an embedding, through intrinsic and extrinsic evaluation.\n",
    "3. Build a SkipGram model, based on your experience on CBOW.\n",
    "4. Extra Credit: implement negative sampling!\n",
    "\n",
    "## 0.3 Instruction\n",
    "\n",
    "To follow along this exercise, you want to execute the cells by clicking on them and press *Run* or hit *Shift+Enter*, from top to bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CBOW\n",
    "\n",
    "\n",
    "Our first task is to build our very own CBOW. In this section, you will **NOT** be required to write code for the network. Instead, you will be exploring the model by forming hypothesis and testing them through different parameter settings. Although you don't need to write code, you are encouraged to explore the model by writing code to test things out.\n",
    "\n",
    "First, let's remind ourselves of the famous quote:\n",
    "\n",
    "> You shall know a word by the company it keeps (Firth, J. R. 1957:11)\n",
    "\n",
    "What this implies is that it is possible to define a word, or *meaning* of a word in a way that describes a prediction task: **the task of predicting the word given the context**. However, one problem still remains: how to represent the meaning of a word? Luckily, there has already been a line of works that suggest a solution: representing the meaning of a word by a vector -- the vector space model.\n",
    "\n",
    "Now let's imagine that we are given a near-perfect vector space model that maps meaning of a word to vector. One of the easiest ways to solve the task then, is that we can literally just use the sum of the context vectors as the inputs, the target word vector as the output, and fit it through a linear model!\n",
    "\n",
    "And that is exactly what CBOW is doing: <img src=\"figures/cbow.png\" alt=\"cobw\" style=\"width: 400px;\"/>\n",
    "\n",
    "The weights between the input and hidden layer are the vector space model mapping, and the weights between the hidden and output layer are the linear prediction model. \n",
    "\n",
    "The only catch is that we don't have that near-perfect vector space model given to us! Thus, our goal is to jointly learn (1) a representation of the word, and (2) a prediction model. In the following sections, we will explore how to do that through PyTorch.\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 Building our training set\n",
    "\n",
    "We want to start with building a training set. For the purpose of this exploratory exercise, you will be using a medium size corpus: the [IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) (you should already downloaded it in `README.md`)\n",
    "\n",
    "\n",
    "### Data Preprocessing\n",
    "\n",
    "First, we removed all punctuations and lowercased everything, and tokenize it by whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility collections for data processing, make sure you already download the data(see README.md)\n",
    "from utils import read_imdb_data\n",
    "\n",
    "# read_imdb_data removes punctuations and lowercase everything\n",
    "X_raw_train, y_train = read_imdb_data('../data/aclImdb/train')\n",
    "raw_text = ' '.join(X_raw_train).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that it is properly loaded, let's peek into the content a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(raw_text[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Vocab\n",
    "\n",
    "For the purpose of this assignment, and for the sake of training time, we limit the vocabulary to the most common 1000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# pick the top words only\n",
    "raw_text_count = Counter(raw_text)\n",
    "vocab = set(list(zip(*raw_text_count.most_common(1000)))[0])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then only keep the selected words in the raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter raw text by vocab\n",
    "text = [r for r in raw_text if r in vocab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 1\n",
    "\n",
    "The preprocessing steps introduced here seem very naive, and potentially problematic. Before you read further down the exercise, based on your understanding of CBOW, \n",
    "\n",
    "1. list three potential concerns with the preprocessing choices, explain why they might be a concern.\n",
    "2. potential fixes for these concerns.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Dataset\n",
    "\n",
    "Now let's build our dataset! First we define some handy helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    '''\n",
    "    helper function to translate context into indexes for inputs\n",
    "    In:\n",
    "        context: a list of words\n",
    "        word_to_ix: a mapping from word to index\n",
    "    Out:\n",
    "        idxs: a list of indexes\n",
    "    '''\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_batcher(X, Y, batches=5):\n",
    "    '''\n",
    "    helper function to batch data, note that batches is not the size of \n",
    "    the batch, but how many batches the dataset is divided into.\n",
    "    In:\n",
    "        X: a matrix of size (num_sample, CONTEXT_SIZE*2)\n",
    "        Y: an array of size (num_sample)\n",
    "        batches: how many batches the dataset is divided into, default:5\n",
    "    Out:\n",
    "        a batch of X and Y\n",
    "    '''\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i in range(batches):\n",
    "        start = int((i * X.shape[0]) / batches)\n",
    "        end = int(((i + 1) * X.shape[0]) / batches)\n",
    "        yield X[start:end, :], Y[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that our given (X) is the context, our target (Y) is the word. \n",
    "\n",
    "We then build our dataset by iterating through the filtered text and mapping the words to indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Let's set the context window size to 2 for now\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "\n",
    "# A mapping from word to index\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "# iterate through the text to build dataset\n",
    "for i in range(CONTEXT_SIZE, len(text) - CONTEXT_SIZE):\n",
    "    context = [text[i + j] for j in range(-CONTEXT_SIZE, CONTEXT_SIZE+1) if not j==0]\n",
    "    target = text[i]\n",
    "    # the data translate to indexes, X is the context, Y is the target word.\n",
    "    X.append(make_context_vector(context, word_to_ix))\n",
    "    Y.append(word_to_ix[target])\n",
    "\n",
    "# convert to numpy for easier data handling later\n",
    "X = np.array(X, dtype=int)\n",
    "Y = np.array(Y, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's investigate what X and Y are a little bit by peeking into them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[837 483 824  26]\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "# Let's print the first X, Y pair\n",
    "x0 = X[0]\n",
    "y0 = Y[0]\n",
    "print(x0)\n",
    "print(y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['story', 'of', 'a', 'man', 'who']\n",
      "[837, 483, 116, 824, 26]\n"
     ]
    }
   ],
   "source": [
    "# x0 is the indexes of the context, while y0 is the index of the target word.\n",
    "print(text[:5])\n",
    "print([word_to_ix[w] for w in text[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 CBOW with PyTorch\n",
    "\n",
    "Now that we have our dataset built, let's import PyTorch!\n",
    "\n",
    "Note: if importing PyTorch failed, first try to click on **Kernel->Change Kernel->Python \\[conda env:pytorch_w2v\\]** on the upper bar of the notebook. It is likely that you did not set the kernel (which is what the jupyter notebook is running on) to the one you had pytorch installed. If you change the kernel, please re-run this jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x144401310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import PyTorch\n",
    "import torch\n",
    "# torch.nn contains modules or subcomponents of the network required to train the network.\n",
    "import torch.nn as nn\n",
    "# torch.nn.functional is a collection of handy functions that you can build into the model\n",
    "import torch.nn.functional as F\n",
    "# torch.optim contains optimizers to update the parameters of network\n",
    "import torch.optim as optim\n",
    "\n",
    "\"\"\"\n",
    "Variable in torch.autograd is used to tell pytorch that \n",
    "the object should be put into the PyTorch computation graph. \n",
    "See readings for more details.\n",
    "\"\"\"\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Let's also set a fix random seed so you can replicate the result\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at two modules of PyTorch that will be especially useful for our implementation of CBOW: `nn.Embedding` and `nn.Linear`\n",
    "\n",
    "Note: In PyTorch, if you donâ€™t specify, the `Tensor` created (and thus the parameters in `nn.Embedding` and `nn.Linear`) will be all random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
      "\n",
      "    This module is often used to store word embeddings and retrieve them using indices.\n",
      "    The input to the module is a list of indices, and the output is the corresponding\n",
      "    word embeddings.\n",
      "\n",
      "    Args:\n",
      "        num_embeddings (int): size of the dictionary of embeddings\n",
      "        embedding_dim (int): the size of each embedding vector\n",
      "        padding_idx (int, optional): If given, pads the output with zeros whenever it encounters the index.\n",
      "        max_norm (float, optional): If given, will renormalize the embeddings to always have a norm lesser than this\n",
      "        norm_type (float, optional): The p of the p-norm to compute for the max_norm option\n",
      "        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the frequency of\n",
      "                                                the words in the mini-batch.\n",
      "        sparse (boolean, optional): if ``True``, gradient w.r.t. weight matrix will be a sparse tensor. See Notes for\n",
      "                                    more details regarding sparse gradients.\n",
      "\n",
      "    Attributes:\n",
      "        weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n",
      "\n",
      "    Shape:\n",
      "        - Input: LongTensor `(N, W)`, N = mini-batch, W = number of indices to extract per mini-batch\n",
      "        - Output: `(N, W, embedding_dim)`\n",
      "\n",
      "    Notes:\n",
      "        Keep in mind that only a limited number of optimizers support\n",
      "        sparse gradients: currently it's `optim.SGD` (`cuda` and `cpu`),\n",
      "        and `optim.Adagrad` (`cpu`)\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> # an Embedding module containing 10 tensors of size 3\n",
      "        >>> embedding = nn.Embedding(10, 3)\n",
      "        >>> # a batch of 2 samples of 4 indices each\n",
      "        >>> input = Variable(torch.LongTensor([[1,2,4,5],[4,3,2,9]]))\n",
      "        >>> embedding(input)\n",
      "\n",
      "        Variable containing:\n",
      "        (0 ,.,.) =\n",
      "         -1.0822  1.2522  0.2434\n",
      "          0.8393 -0.6062 -0.3348\n",
      "          0.6597  0.0350  0.0837\n",
      "          0.5521  0.9447  0.0498\n",
      "\n",
      "        (1 ,.,.) =\n",
      "          0.6597  0.0350  0.0837\n",
      "         -0.1527  0.0877  0.4260\n",
      "          0.8393 -0.6062 -0.3348\n",
      "         -0.8738 -0.9054  0.4281\n",
      "        [torch.FloatTensor of size 2x4x3]\n",
      "\n",
      "        >>> # example with padding_idx\n",
      "        >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n",
      "        >>> input = Variable(torch.LongTensor([[0,2,0,5]]))\n",
      "        >>> embedding(input)\n",
      "\n",
      "        Variable containing:\n",
      "        (0 ,.,.) =\n",
      "          0.0000  0.0000  0.0000\n",
      "          0.3452  0.4937 -0.9361\n",
      "          0.0000  0.0000  0.0000\n",
      "          0.0706 -2.1962 -0.6276\n",
      "        [torch.FloatTensor of size 1x4x3]\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(nn.Embedding.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
      "\n",
      "    Args:\n",
      "        in_features: size of each input sample\n",
      "        out_features: size of each output sample\n",
      "        bias: If set to False, the layer will not learn an additive bias.\n",
      "            Default: ``True``\n",
      "\n",
      "    Shape:\n",
      "        - Input: :math:`(N, *, in\\_features)` where `*` means any number of\n",
      "          additional dimensions\n",
      "        - Output: :math:`(N, *, out\\_features)` where all but the last dimension\n",
      "          are the same shape as the input.\n",
      "\n",
      "    Attributes:\n",
      "        weight: the learnable weights of the module of shape\n",
      "            (out_features x in_features)\n",
      "        bias:   the learnable bias of the module of shape (out_features)\n",
      "\n",
      "    Examples::\n",
      "\n",
      "        >>> m = nn.Linear(20, 30)\n",
      "        >>> input = autograd.Variable(torch.randn(128, 20))\n",
      "        >>> output = m(input)\n",
      "        >>> print(output.size())\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(nn.Linear.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The CBOW model\n",
    "\n",
    "Now, it's the exciting part. Let's build our model!\n",
    "\n",
    "Recall that network structure of CBOW is equivalent to $A\\left ( \\sum_{w \\in Context} q_w \\right ) + b$, where $q_w$ is the vector representation of word $w$, A and b are parameters for the linear prediction model. If we applied log softmax to convert the output to log probability, the output of network then becomes: \n",
    "\n",
    "$$\\log P(w_i|context) = logSoftmax\\left (A\\left ( \\sum_{w \\in context} q_w \\right ) + b\\right )$$\n",
    "\n",
    "Our goal is then\n",
    "\n",
    "$$arg\\min_{A, b, Q} E_{(context, target) \\sim D} \\left [-\\log P(w_{target}|context)\\right ] $$\n",
    "\n",
    "where $Q$ is the matrix of embeddings, and D is a distribution that context and target words are drawn from.\n",
    "\n",
    "\n",
    "With this in mind, our model is thus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of CBOW for exploratory purpose.\n",
    "    \n",
    "    Args:\n",
    "        - vocab_size: size of the vocabulary\n",
    "        - embedding_dim: dimension of the representation vector for words\n",
    "        - word_to_ix: a mapping from word to index\n",
    "    \n",
    "    Shape:\n",
    "        - Input: LongTensor (N, W), N = mini-batch size, \n",
    "                 W = number of indices to extract per mini-batch\n",
    "        - Output: (N, vocab_size),  N = mini-batch size\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Initializing the model, instantiating the required module (Not linking them)\n",
    "    def __init__(self, vocab_size, embedding_dim, word_to_ix):\n",
    "        # A standard python way of saying CBOW is going to inherit nn.Module\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        self.word_to_ix = word_to_ix\n",
    "        \n",
    "        \"\"\"\n",
    "        We create an nn.Embedding instance with vocab size and embedding \n",
    "        dimension specified. sparse=True enable sparse gradient updates, \n",
    "        which speed up the computation and save memory.\n",
    "        \"\"\"\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
    "       \n",
    "        \"\"\"\n",
    "        We create an nn.Linear instance with embedding_dim as the input\n",
    "        features size and vocab_size as the output size. Recall that \n",
    "        the goal of nn.Linear is a prediction model which maps from an \n",
    "        embedding to scores over the vocabulary space.\n",
    "        \"\"\"\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        \"\"\"\n",
    "        We create an nn.LogSoftmax instance that convert output of \n",
    "        self.linear to log probility over dim=1. dim here stands for \n",
    "        dimension.\n",
    "\n",
    "        Just like numpy, PyTorch's Tensor is also an n-dimensional array.\n",
    "        Since the input we will pass in at a time is a batch of data points, \n",
    "        the first dimension (dim=0) is the batch dimension (can be \n",
    "        understood as a list of data point). Thus, we need to tell LogSoftmax\n",
    "        that dim=1 is the actual data point.\n",
    "        \"\"\"\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    # Here is where we acutally link the modules to describe how the data flow through the network.\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        - inputs: LongTensor (N, W), N = mini-batch size, \n",
    "                  W = number of indices to extract per mini-batch\n",
    "        - outputs: (N, vocab_size),  N = mini-batch size\n",
    "        \"\"\"\n",
    "        out = self.emb(inputs).sum(dim=1)\n",
    "        out = self.linear(out)\n",
    "        return self.logsoftmax(out)\n",
    "\n",
    "    # helper function to retrieve the trained vector space model (or word embedding)\n",
    "    def word_embedding(self):\n",
    "        return self.emb.weight\n",
    "    \n",
    "    # helper function to do a word to vector lookup\n",
    "    def word2vec(self, word):\n",
    "        return self.emb.weight[self.word_to_ix[word], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there really wasn't many lines of code! Most of those are acutally my comments!\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "Next we need to actually train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handy library to help you visualize the progress\n",
    "import progressbar\n",
    "\n",
    "def train_cbow(model, num_epochs=5, batches=100):\n",
    "    \"\"\"\n",
    "    A function to call for training given the model.\n",
    "\n",
    "    Parameters for train_cbow():\n",
    "        - model: a CBOW instance\n",
    "        - num_epochs: number of time the entire dataset is trained through\n",
    "        - batches:  recall that batches is not the size of the batch, \n",
    "                    but how many batches the dataset is divided into.\n",
    "    \"\"\" \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    The negative log likelihood loss. \n",
    "    It is useful to train a classification problem with C classes.\n",
    "    Expected to contain log-probabilities of each class. \n",
    "    See nn.NLLLoss.__doc__ for more details.\n",
    "    \"\"\"\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    \"\"\"\n",
    "    The stochastic gradient descent optimizer used to update weights, \n",
    "    once gradient is computed. We set a learning rate of 0.001, and \n",
    "    momentum of 0.9.\n",
    "    \"\"\"\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # keep track of the loss of the model to see if the model converges.\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # progressbar setting\n",
    "        widgets = ['Epoch {} '.format(epoch), progressbar.Percentage(), ' ', progressbar.Bar(), ' ', progressbar.ETA()]\n",
    "        bar = progressbar.ProgressBar(widgets=widgets, max_value=batches)\n",
    "\n",
    "        # bar(data_batcher(X, Y, batches)) is a way for progressbar to keep track of data_batcher(X, Y, batches)\n",
    "        for context, target in bar(data_batcher(X, Y, batches)):\n",
    "            \n",
    "            \"\"\"\n",
    "            Wrap our training pair context and target first through torch.LongTensor, \n",
    "            and then through Variable.\n",
    "            \n",
    "            torch.LongTensor is a holder for integer variable, and is expected by the model as input.\n",
    "            \n",
    "            Variable is used to put x and y into the PyTorch computation graph. See readings for more details.\n",
    "            \"\"\"\n",
    "            x = Variable(torch.LongTensor(context))\n",
    "            y = Variable(torch.LongTensor(target))\n",
    "            \n",
    "            \"\"\"\n",
    "            zero the parameter gradients\n",
    "        \n",
    "            If you don't do so, the gradients will accumulate and lead to significant slow down.\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \"\"\"\n",
    "            forward\n",
    "            \n",
    "            The final piece how connecting the network to the data! \n",
    "            We feed the data in, and get a loss value back.\n",
    "            \"\"\"\n",
    "            # run this input x forward through the network to get your output vector\n",
    "            outputs = model(x)\n",
    "            # compare outputs against correct output y and generate loss using the loss function\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            \"\"\"\n",
    "            backward\n",
    "            \n",
    "            This may seem like black magic, but what it really does is \n",
    "            essentially doing chain rule (or a fancier term -- backpropagation)\n",
    "            \n",
    "            Intuitively, for every object x that is a Variable, this calculates\n",
    "            \n",
    "            d loss\n",
    "            -------\n",
    "            d x\n",
    "            \n",
    "            And exactly how to break down the above term to get the actual gradient?\n",
    "            This is done automatically by the computation graph PyTorch build while \n",
    "            you write the model! Thus the name 'autograd'.\n",
    "            \"\"\"\n",
    "            loss.backward()\n",
    "            \n",
    "            \"\"\"\n",
    "            optimize\n",
    "            \n",
    "            Now that for every Variable, we have calculate the gradient, we can use an\n",
    "            optimizer to update the weights!\n",
    "            \n",
    "            SGD for example, does something intuitively like this:\n",
    "            w = w + learning_rate*w.grad\n",
    "            \"\"\"\n",
    "            optimizer.step()\n",
    "            \n",
    "            # just to record the loss\n",
    "            losses.append(loss.data.numpy())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib to plot the losses\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initalize a CBOW model of vocab_size 1000 (the top 1000 words we limited it to) and train a vector space model with embbeding dimension=10, for the sake of shortening the training time. Note that a dimension=10 may not be optimal, and will be a parameter that you can potentially tune in Question 2.\n",
    "\n",
    "We then train the model for 3 epoches, with 100,000 data batches. These are also parameters that you can potentially tune.\n",
    "\n",
    "This [link](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network) provides a nice discussion between data batches and iterations. [Here](https://stackoverflow.com/questions/4752626/epoch-vs-iteration-when-training-neural-networks) is a discussion for the definition of epoch, iteration, and data batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 100% |##################################################| Time: 0:03:17\n",
      "Epoch 1 100% |##################################################| Time: 0:03:16\n",
      "Epoch 2 100% |##################################################| Time: 0:03:24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13a1fbef0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128282b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CBOW(vocab_size=vocab_size, embedding_dim=10, word_to_ix=word_to_ix)\n",
    "\n",
    "# Train with 3 epochs, 100,000 data batches\n",
    "plt.plot(train_cbow(model, num_epochs=3, batches=100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluation\n",
    "\n",
    "Now that we have a model trained, how do we know if it is good at all? There are generally two ways of evaluating a vector space model: intrinsic and extrinsic evaluation.\n",
    "\n",
    "\n",
    "## 2.1 Intrinsic Evaluation\n",
    "\n",
    "### Sanity Check\n",
    "\n",
    "**Sanity check**, or **smell test**, is a quick and dirty way to check if a model is doing anything reasonable at all. The idea of a sanity check is to check for a property that a good model should certainly hold. In our case, since we are training on a movie review dataset, we can safely assume that *'movie'* and *'film'* should have similar representation in a reasonably well-trained model, as they should appear in similar context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9103\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is PyTorch's cosine similarity module\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "# The word 'movie' and 'film' should appear in similar context, and thus should have similar representation.\n",
    "cos(model.word2vec('movie'), model.word2vec('film'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the 10 closest words to *movie*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_n_words(model, vocab, word, n=10):\n",
    "    cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    word_vec = model.word2vec(word)\n",
    "    scores=[]\n",
    "    for v in vocab:\n",
    "        if not v == word:\n",
    "            score = cos(model.word2vec(v), word_vec)\n",
    "            scores.append((score, v))\n",
    "    \n",
    "    \"\"\"\n",
    "    return the n closest words to the target word\n",
    "\n",
    "    The purpose of using score[0].data.numpy() is to convert Tensor into\n",
    "    numpy array, since there are many things you cannot easily do to a\n",
    "    Tensor (like comparing two value and return a boolean, since it is \n",
    "    not differentiable!)\n",
    "    \"\"\"\n",
    "    n_closest = list(zip(*(sorted(scores, key=lambda score: score[0].data.numpy(), reverse=True)[:n])))[1]\n",
    "    return n_closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('film',\n",
       " 'alone',\n",
       " 'up',\n",
       " 'important',\n",
       " 'anything',\n",
       " 'monster',\n",
       " 'drama',\n",
       " 'police',\n",
       " 'however',\n",
       " 'words')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_n_words(model, vocab, 'movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity Dataset\n",
    "\n",
    "The intuition that a good vector space model should rank words that are similar closer to the top, and words that are dissimilar closer to the bottom, is what is behind word similarity test.\n",
    "\n",
    "A data point in a word similarity dataset often consists of 2 words and a score of how related those 2 words are, according to human judgement.\n",
    "\n",
    "Here's a peek into [WS353](http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>ws_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>media</td>\n",
       "      <td>radio</td>\n",
       "      <td>7.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bread</td>\n",
       "      <td>butter</td>\n",
       "      <td>6.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cucumber</td>\n",
       "      <td>potato</td>\n",
       "      <td>5.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doctor</td>\n",
       "      <td>nurse</td>\n",
       "      <td>7.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>professor</td>\n",
       "      <td>doctor</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>student</td>\n",
       "      <td>professor</td>\n",
       "      <td>6.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>smart</td>\n",
       "      <td>stupid</td>\n",
       "      <td>5.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wood</td>\n",
       "      <td>forest</td>\n",
       "      <td>7.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>money</td>\n",
       "      <td>cash</td>\n",
       "      <td>9.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>king</td>\n",
       "      <td>queen</td>\n",
       "      <td>8.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word1      word2  ws_score\n",
       "0        tiger        cat      7.35\n",
       "1        tiger      tiger     10.00\n",
       "2        plane        car      5.77\n",
       "3        train        car      6.31\n",
       "4   television      radio      6.77\n",
       "5        media      radio      7.42\n",
       "6        bread     butter      6.19\n",
       "7     cucumber     potato      5.92\n",
       "8       doctor      nurse      7.00\n",
       "9    professor     doctor      6.62\n",
       "10     student  professor      6.81\n",
       "11       smart     stupid      5.81\n",
       "12        wood     forest      7.73\n",
       "13       money       cash      9.15\n",
       "14        king      queen      8.58"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_ws353sim = pd.read_csv('../data/en/EN-WS-353-SIM.txt', sep='\\t', header=None, names=['word1', 'word2', 'ws_score'])\n",
    "df_ws353sim.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can also generate cosine similarity scores for all these word pairs (if available) using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>ws_score</th>\n",
       "      <th>cbow_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>media</td>\n",
       "      <td>radio</td>\n",
       "      <td>7.42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bread</td>\n",
       "      <td>butter</td>\n",
       "      <td>6.19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cucumber</td>\n",
       "      <td>potato</td>\n",
       "      <td>5.92</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>doctor</td>\n",
       "      <td>nurse</td>\n",
       "      <td>7.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>professor</td>\n",
       "      <td>doctor</td>\n",
       "      <td>6.62</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word1   word2  ws_score  cbow_score\n",
       "0       tiger     cat      7.35         NaN\n",
       "1       tiger   tiger     10.00         NaN\n",
       "2       plane     car      5.77         NaN\n",
       "3       train     car      6.31         NaN\n",
       "4  television   radio      6.77         NaN\n",
       "5       media   radio      7.42         NaN\n",
       "6       bread  butter      6.19         NaN\n",
       "7    cucumber  potato      5.92         NaN\n",
       "8      doctor   nurse      7.00         NaN\n",
       "9   professor  doctor      6.62         NaN"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_similarity(row, model, vocab, cos):\n",
    "    word1 = row['word1'].lower()\n",
    "    word2 = row['word2'].lower()\n",
    "    if word1 in vocab and word2 in vocab:\n",
    "        return cos(model.word2vec(word1), model.word2vec(word2)).data.numpy()[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df_ws353sim['cbow_score'] = df_ws353sim.apply(lambda row: cosine_similarity(row, model, vocab, cos), axis=1)\n",
    "df_ws353sim.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a relatively small vocab size, many of the words will be missing, let's drop these row for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>ws_score</th>\n",
       "      <th>cbow_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>American</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.114776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>life</td>\n",
       "      <td>death</td>\n",
       "      <td>7.88</td>\n",
       "      <td>-0.062488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>type</td>\n",
       "      <td>kind</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.281635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>street</td>\n",
       "      <td>place</td>\n",
       "      <td>6.44</td>\n",
       "      <td>-0.129557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>man</td>\n",
       "      <td>woman</td>\n",
       "      <td>8.30</td>\n",
       "      <td>-0.759632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>street</td>\n",
       "      <td>children</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.193394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>start</td>\n",
       "      <td>year</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.228381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>theater</td>\n",
       "      <td>history</td>\n",
       "      <td>3.91</td>\n",
       "      <td>-0.269554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>experience</td>\n",
       "      <td>music</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.097135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2  ws_score  cbow_score\n",
       "64     Japanese  American      6.50    0.114776\n",
       "70         life     death      7.88   -0.062488\n",
       "72         type      kind      8.97    0.281635\n",
       "73       street     place      6.44   -0.129557\n",
       "90          man     woman      8.30   -0.759632\n",
       "103      street  children      4.94    0.193394\n",
       "125       start      year      4.06    0.228381\n",
       "128     theater   history      3.91   -0.269554\n",
       "144  experience     music      3.47    0.097135"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws353sim_dropna = df_ws353sim.dropna().copy()\n",
    "df_ws353sim_dropna.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, a good model would generate a similar scores compared to that of humans.\n",
    "\n",
    "However, it is relatively hard to compare 2 variable with different range. A common approach then, is to transform the scores into rank before comparing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>ws_score</th>\n",
       "      <th>cbow_score</th>\n",
       "      <th>ws_rank</th>\n",
       "      <th>cbow_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>American</td>\n",
       "      <td>6.50</td>\n",
       "      <td>0.114776</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>life</td>\n",
       "      <td>death</td>\n",
       "      <td>7.88</td>\n",
       "      <td>-0.062488</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>type</td>\n",
       "      <td>kind</td>\n",
       "      <td>8.97</td>\n",
       "      <td>0.281635</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>street</td>\n",
       "      <td>place</td>\n",
       "      <td>6.44</td>\n",
       "      <td>-0.129557</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>man</td>\n",
       "      <td>woman</td>\n",
       "      <td>8.30</td>\n",
       "      <td>-0.759632</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>street</td>\n",
       "      <td>children</td>\n",
       "      <td>4.94</td>\n",
       "      <td>0.193394</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>start</td>\n",
       "      <td>year</td>\n",
       "      <td>4.06</td>\n",
       "      <td>0.228381</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>theater</td>\n",
       "      <td>history</td>\n",
       "      <td>3.91</td>\n",
       "      <td>-0.269554</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>experience</td>\n",
       "      <td>music</td>\n",
       "      <td>3.47</td>\n",
       "      <td>0.097135</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1     word2  ws_score  cbow_score  ws_rank  cbow_rank\n",
       "64     Japanese  American      6.50    0.114776        4          4\n",
       "70         life     death      7.88   -0.062488        3          6\n",
       "72         type      kind      8.97    0.281635        1          1\n",
       "73       street     place      6.44   -0.129557        5          7\n",
       "90          man     woman      8.30   -0.759632        2          9\n",
       "103      street  children      4.94    0.193394        6          3\n",
       "125       start      year      4.06    0.228381        7          2\n",
       "128     theater   history      3.91   -0.269554        8          8\n",
       "144  experience     music      3.47    0.097135        9          5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ws353sim_dropna['ws_rank'] = df_ws353sim_dropna['ws_score'].rank(ascending=False).astype(int)\n",
    "df_ws353sim_dropna['cbow_rank'] = df_ws353sim_dropna['cbow_score'].rank(ascending=False).astype(int)\n",
    "df_ws353sim_dropna.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But exactly how do we measure how *close* two rankings are? The answer is that we want to calculate the [Spearman's rho correlation coefficient](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient), which is measuring the rank correlation of 2 variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.666666666666667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\"\"\"\n",
    "calculating spearman's rho using scipy.stats\n",
    "Spearman's rho range from [-1, 1]\n",
    "we *100 to scale it to [-100, 100]\n",
    "\"\"\"\n",
    "stats.stats.spearmanr(df_ws353sim_dropna['ws_rank'], df_ws353sim_dropna['cbow_rank'])[0]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By repeating this process for all the other datasets located in `data/en/`, you can get a fairly overall feel of how the model is doing compared to human judgements.\n",
    "\n",
    "For you convenience, here I have provided a class (Wordsim) that automatically test an array of word similarity datasets, taken (modified to work on Python 3 and PyTorch) from Kazuya Kawakami's [embedding-evaluation](https://github.com/k-kawakami/embedding-evaluation) code.\n",
    "\n",
    "What these datasets are and their properties are also described in the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.55316603,  0.50568599,  0.15468752, -0.32579049,  2.10974979,\n",
       "       -0.56246012, -0.84836906, -2.8567605 ,  0.05953474,  1.67701232], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import Wordsim\n",
    "\n",
    "wordsim = Wordsim()\n",
    "\"\"\"\n",
    "convert model to a dictionary of word to numpy array, \n",
    "the model must have a model.word_to_ix variable\n",
    "\"\"\"\n",
    "word2vec = wordsim.convert_to_w2v(model)\n",
    "word2vec['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-----------+----------------+\n",
      "| Dataset        | Found | Not Found |  Score (rho)   |\n",
      "+----------------+-------+-----------+----------------+\n",
      "| EN-MC-30       |   0   |     30    |      nan       |\n",
      "| EN-MEN-TR-3k   |  132  |    2868   | -4.68663005455 |\n",
      "| EN-MTurk-287   |   6   |    281    | -65.7142857143 |\n",
      "| EN-MTurk-771   |   27  |    744    | -13.5531135531 |\n",
      "| EN-RG-65       |   0   |     65    |      nan       |\n",
      "| EN-RW-STANFORD |   3   |    2031   |     -50.0      |\n",
      "| EN-WS-353-ALL  |   18  |    335    | 18.0598555212  |\n",
      "| EN-WS-353-REL  |   13  |    239    | 42.8571428571  |\n",
      "| EN-WS-353-SIM  |   9   |    194    | 6.66666666667  |\n",
      "| EN-YP-130      |   0   |    130    |      nan       |\n",
      "+----------------+-------+-----------+----------------+\n"
     ]
    }
   ],
   "source": [
    "result = wordsim.evaluate(word2vec)\n",
    "wordsim.pprint(result)  # scores reported here are Spearman's rho * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Extrinsic Evaluation\n",
    "\n",
    "While a high score on intrinsic evaluation reflects that we captured important linguistic information, sometime what we really want is to do well on a downstream task.\n",
    "\n",
    "For the purpose of this assignment, let's assume that we want to evaluate it with the IMDB dataset! (A binary document classification task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the train and test data\n",
    "X_raw_train, y_train = read_imdb_data('../data/aclImdb/train')\n",
    "X_raw_test, y_test = read_imdb_data('../data/aclImdb/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'story of a man who has unnatural feelings for a pig starts out with a opening scene that is a terrific example of absurd comedy a formal orchestra audience is turned into an insane violent mob by the crazy chantings of its singers unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting even those from the era should be turned off the cryptic dialogue would make shakespeare seem easy to a third grader on a technical level its better than you might think with some good cinematography by future great vilmos zsigmond future stars sally kirkland and frederic forrest can be seen briefly'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our goal is to transform the document into a dense representation vector related to the embedding matrix. One of the easiest way to do that is just by averaging all the word vectors in the document. \n",
    "\n",
    "\n",
    "To efficently do that, let's start by converting the document to an normalized bag-of-words style vector, using `CountVectorizer` and `normalize` in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vocabulary=word_to_ix ensure that we have the exact same set of vocab and index mapping.\n",
    "vectorizer = CountVectorizer(vocabulary=word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sidshing/anaconda2/envs/pytorch-rg/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# tranform train and test to bag-of-words style\n",
    "X_train_bow = vectorizer.transform(X_raw_train)\n",
    "X_test_bow = vectorizer.transform(X_raw_test)\n",
    "\n",
    "# let's also l1-normalize the bag-of-word vectors by count\n",
    "from sklearn.preprocessing import normalize\n",
    "X_train_norm = normalize(X_train_bow, axis=1, norm='l1')\n",
    "X_test_norm = normalize(X_test_bow, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embedding_matrix, turn in into numpy matrix\n",
    "embedding_matrix = model.word_embedding().data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next step, we need to transform the normalized bag-of-words vector by the embedding matrix. We can easily do that by:\n",
    "\n",
    "$$X_{emb} = X_{bow} \\Phi$$\n",
    "\n",
    "where $X_{emb}$ is the dense representation of documents, $X_{bow}$ is the normalized bag-of-words representation of documents, and $\\Phi$ is the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.matmul(X_train_norm.todense(), embedding_matrix)\n",
    "X_test = np.matmul(X_test_norm.todense(), embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Logistic Regression as our learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58316000000000001"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Question 2\n",
    "\n",
    "Now that you have learned how to evaluate a vector space model, let's explore the model by changing the parameters!\n",
    "\n",
    "For purposes of this question, \"changing the parameters\" can be interpreted broadly: it can include numeric parameters of the model (e.g. the number of dimensions in your embedding), parameters related to the training (e.g. batch size, number of training epochs), and even choices about preprocessing (e.g. decisions that were made about normalization, filtering out of vocabulary, etc.).\n",
    "\n",
    "Form **two hypotheses** about parameter choices and how they are affecting the modeling. For each hypothesis, do some experimentations to explore whether your hypothesis is true or not. In your writeup, for each one explain the hypothesis (and its rationale or the intuitions behind it), your experiments, and any results you obtained. There is no need to include code in your writeup.\n",
    "\n",
    "Note that you may not be able to successfully test a hypothesis, either due to long training time or lack of training data. If that happens, instead of experiments and results, talk about what you tried, give an explanation of why it didn't work, and describe what you would have expected to see if the experiment had run successfully.  What's important is showing that you've grounded your answer to this question in a clear understanding of the material; for example, there should be a clear logic to why an experiment you propose would support or refute your hypothesis, even if you don't get to run the experiment.\n",
    "\n",
    "Also note that we are not looking for state of the art performance, in terms of the quality of embeddings (you don't even have to beat the current performance, which is a very low bar). The central goal is for you get hands on and get a feel for how this model behaves using some real data.\n",
    "That said, you are welcome to go beyond what we've provided here, e.g. you could get a model working on a small dataset, and then run it on some larger dataset that is of interest to you. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SkipGram\n",
    "\n",
    "The third task is to write codes to train a SkipGram model. Take inspiration from the CBOW classifier trained previously, but remember the implementational differences in SkipGram.\n",
    "\n",
    "SkipGram, in contrast to CBOW, is trying to predict the context word given the target word:\n",
    "\n",
    "<img src=\"figures/skipgram.png\" alt=\"skipgram\" style=\"width: 400px;\"/>\n",
    "\n",
    "Here is a nice tutorial for SkipGram: [Word2Vec Tutorial: The SkipGram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "\n",
    "\n",
    "Another crucial difference is how you process the data, note that you don't just flip the X and Y. Take the sentence `\"was the best of time\"` for example, for CBOW, this create 1 training pair:\n",
    "\n",
    "`(X: ['was', 'the', 'of', 'time'], Y: 'best')`\n",
    "\n",
    "But for SkipGram, this would actually create 4 training pairs:\n",
    "\n",
    "1. `(X: ['best'], Y: 'was')`\n",
    "2. `(X: ['best'], Y: 'the')`\n",
    "3. `(X: ['best'], Y: 'of')`\n",
    "4. `(X: ['best'], Y: 'time')`\n",
    "\n",
    "That is, you split the context vector (what you are trying to predict for SkipGram) into single words. Obviously, these training data pairs have to be translate into indexes before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to write code that takes in the dataset and creates \n",
    "# the training examples from it.\n",
    "\n",
    "# Let's set the context window size to 2 for now\n",
    "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "# iterate through the text to build dataset\n",
    "for i in range(CONTEXT_SIZE, len(text) - CONTEXT_SIZE):\n",
    "    context = [text[i + j] for j in range(-CONTEXT_SIZE, CONTEXT_SIZE+1) if not j==0]\n",
    "    target = text[i]\n",
    "    # the data translate to indexes, X is the context, Y is the target word.\n",
    "    for word in context:\n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \n",
    "        Fill in this part of the code for creating the dataset. \n",
    "        Remember to use word_to_ix (define in the preprocessing of CBOW)\n",
    "        to transform word to index.\n",
    "        \n",
    "        The shape of X and Y (and also how you process them here) depend\n",
    "        on your implementation of SkipGram, as X and Y would be fed into \n",
    "        the SkipGram model in batches.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# convert to numpy for easier data handling later\n",
    "X = np.array(X, dtype=int)\n",
    "Y = np.array(Y, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get to actually building the SkipGram model. For the purpose of this assignment, you do **NOT** need to implement negative sampling or hierarchical softmax. Actually, implementing negative sampling will be the extra credit. All you need to do is follow the same log softmax approach to get a probability distribution.\n",
    "\n",
    "Some hints: notice that CBOW and SkipGram have very similar structures. Also, depending on how you built your dataset, your network may or may not need to change. You might also need to rewrite `data_batcher` to fit your need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch implementation of SkipGram for exploratory purpose.\n",
    "    \n",
    "    Args:\n",
    "        - vocab_size: size of the vocabulary\n",
    "        - embedding_dim: dimension of the representation vector for words\n",
    "        - word_to_ix: a mapping from word to index\n",
    "    \n",
    "    Shape:\n",
    "        - Input: Depends on your dataset\n",
    "        - Output: (N, vocab_size),  N = mini-batch size\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Initializing the model, instantiating the required module (Not linking them)\n",
    "    def __init__(self, vocab_size, embedding_dim, word_to_ix):\n",
    "        # A standard python way of saying SkipGram is going to inherit nn.Module\n",
    "        super(SkipGram, self).__init__()\n",
    "        \n",
    "        self.word_to_ix = word_to_ix\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO\n",
    "        \n",
    "        Just like CBOW, you may want to use nn.Embedding, nn.Linear, and/or nn.LogSoftmax\n",
    "        \"\"\"\n",
    "        \n",
    "        self.emb = None\n",
    "       \n",
    "\n",
    "    # Here is where we acutally link the modules to describe how the data flow through the network.\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        - inputs: Depends on your dataset\n",
    "        - outputs: (N, vocab_size),  N = mini-batch size\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    # helper function to retrieve the trained vector space model (or word embedding)\n",
    "    def word_embedding(self):\n",
    "        return self.emb.weight\n",
    "    \n",
    "    # helper function to do a word to vector lookup\n",
    "    def word2vec(self, word):\n",
    "        return self.emb.weight[self.word_to_ix[word], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've created the model, you'll need to train it. Here is a skeleton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(model, num_epochs=5, batches=100):\n",
    "    \"\"\"\n",
    "    A function to call for training given the model.\n",
    "\n",
    "    Parameters for train_skipgram():\n",
    "        - model: a SkipGram instance\n",
    "        - num_epochs: number of time the entire dataset is trained through\n",
    "        - batches:  recall that batches is not the size of the batch, \n",
    "                    but how many batches the dataset is divided into.\n",
    "    \"\"\" \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \n",
    "    choose a loss function\n",
    "    \"\"\"\n",
    "    criterion = None\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \n",
    "    choose a optimizer\n",
    "    \"\"\"\n",
    "    optimizer = None\n",
    "\n",
    "    # keep track of the loss of the model to see if the model converges.\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # progressbar setting\n",
    "        widgets = ['Epoch {} '.format(epoch), progressbar.Percentage(), ' ', progressbar.Bar(), ' ', progressbar.ETA()]\n",
    "        bar = progressbar.ProgressBar(widgets=widgets, max_value=batches)\n",
    "\n",
    "        # bar(data_batcher(X, Y, batches)) is a way for progressbar to keep track of data_batcher(X, Y, batches)\n",
    "        # data_batcher might need to be rewrited, depending out how you build your dataset.\n",
    "        for target, context in bar(data_batcher(X, Y, batches)):\n",
    "            \n",
    "            \"\"\"\n",
    "            TODO\n",
    "            \n",
    "            Remember to wrap you x and y with Variable\n",
    "            \"\"\"\n",
    "            x = None\n",
    "            y = None\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            zero the parameter gradients\n",
    "            If you don't do so, the gradients will accumulate and lead to significant slow down.\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            \"\"\"\n",
    "            TODO\n",
    "            \n",
    "            Feed the data in, and get a loss value back.\n",
    "            \"\"\"\n",
    "            # run this input x forward through the network to get your output vector\n",
    "            outputs = None\n",
    "            # compare outputs against correct output y and generate loss using the loss function\n",
    "            loss = None\n",
    "            \n",
    "            \"\"\"\n",
    "            Optional TODO: modify this part if needed\n",
    "            \n",
    "            backward\n",
    "            \"\"\"\n",
    "            loss.backward()\n",
    "            \n",
    "            \"\"\"\n",
    "            Optional TODO: modify this part if needed\n",
    "            \n",
    "            optimize\n",
    "            \"\"\"\n",
    "            optimizer.step()\n",
    "            \n",
    "            # just to record the loss\n",
    "            losses.append(loss.data.numpy())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train your model! (Feel free to change this piece of code to fit your need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(vocab_size=vocab_size, embedding_dim=10, word_to_ix=word_to_ix)\n",
    "\n",
    "# Train with 3 epochs, 100,000 data batches\n",
    "plt.plot(train_skipgram(model, num_epochs=3, batches=100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to evaluate it! Here is a piece of code for the smell test. You are free to evaluate it with any other method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is PyTorch's cosine similarity module\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "\n",
    "# The word 'movie' and 'film' should appear in similar context, and thus should have similar representation.\n",
    "cos(model.word2vec('movie'), model.word2vec('film'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Question 3\n",
    "\n",
    "Now that you have implemented SkipGram, please describe to us how you did it!\n",
    "\n",
    "Do **NOT** just paste all your code inside the writeup, write down your logic for why you choose to design the specific component the way it is. The documentation we wrote with CBOW might be a good reference. Although we discourage you to paste all your code, but if snippet (a few lines) of code helps with the explanation, feel free to include it!\n",
    "\n",
    "You need to at least describe your code in 4 parts:\n",
    "\n",
    "1. How you build the dataset\n",
    "2. Implementation of SkipGram module\n",
    "3. Implementation of train_skipgram()\n",
    "4. The choice of your SkipGram parameters, and why\n",
    "\n",
    "Other discussions are definitely welcome! \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Question 4\n",
    "\n",
    "Finally, evaluate your SkipGram model with the sanity check plus another method. Describe the evaluation method as well as the result. Again, we are not looking for any state of the art here, but you should at least pass the sanity check. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit: Negative Sampling (20%)\n",
    "\n",
    "Finally, for those of you who want to take on the challenge, I encourage you to attempt to implement negative sampling! Recall the negative sampling loss is defined as:\n",
    "\n",
    "$$\\log \\sigma \\left ( {q^{\\prime}_{w_O}}^\\top q_{w_I} \\right ) + \\sum^{k}_{i=1} E_{w_i \\sim P_n \\left ( w \\right )}\\left [ \\log \\sigma \\left ( -{q^{\\prime}_{w_i}}^\\top q_{w_I} \\right ) \\right ]$$\n",
    "\n",
    "in the word2vec [reading](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). $q^\\prime$ refers to the output embedding, whereas $q$ referes to the input embedding. See reading for more details\n",
    "\n",
    "Your two options are to write a general purpose negative sampling loss module, or to wrap the whole thing up into the implementation of SkipGram. \n",
    "\n",
    "Hint: the loss function of PyTorch is really just another `nn.Module` that returns a scalar (more accurately, a batch of scalars). You might also want to consider using 2 `nn.Embedding` modules, as you no longer need to perform softmax over the vocabulary space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingSkipgram(nn.Module):\n",
    "    \"\"\"\n",
    "    Your code goes here. \n",
    "    You are free to implement it any way you want\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Your code goes here.\n",
    "# Remember to instantiate a model, loss function, and optimizer\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    # Your code goes here.\n",
    "    # You may use the data_batcher function, but remember to wrap your batches\n",
    "    # in a Variable()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Question Extra Credit\n",
    "\n",
    "Describe what you did with `NegativeSamplingSkipgram` just like Question 3. You only need to be brief and describe the core concept.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-rg]",
   "language": "python",
   "name": "conda-env-pytorch-rg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
